# Reference links

## Seq2Seq & Attention
- [Stanford CS224N: NLP with Deep Learning | Winter 2019 | Lecture 8 â€“ Translation, Seq2Seq, Attention](https://www.youtube.com/watch?v=XXtpJxZBa2c&list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&index=8)

Onboarding docs links 
- https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/ 
- https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html 


## Word Embeddings
- https://medium.com/deeper-learning/glossary-of-deep-learning-word-embedding-f90c3cec34ca


## Transformers
- Basics about Transformers (HuggingFace): https://huggingface.co/learn/nlp-course/chapter1/1
- Self-attention in BERT: https://towardsdatascience.com/understand-self-attention-in-bert-intuitively-cd480cbff30b
- Basic Overview: https://jalammar.github.io/illustrated-transformer/ 
- https://towardsdatascience.com/transformers-explained-visually-not-just-how-but-why-they-work-so-well-d840bd61a9d3 
- https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0 
- https://data-science-blog.com/blog/2021/04/07/multi-head-attention-mechanism/ 
- https://data-science-blog.com/blog/2021/04/22/positional-encoding-residual-connections-padding-masks-all-the-details-of-transformer-model/ 
- CS480/680 Lecture 19: Attention and Transformer Networks - https://youtu.be/OyFJWRnt_AY


## RNN
- https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21#:~:text=A%20tanh%20function%20ensures%20that,allowed%20by%20the%20tanh%20function.&text=So%20that's%20an%20RNN

